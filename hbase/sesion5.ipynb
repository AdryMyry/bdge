{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NoSQL (HBase) (sesión 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of HBase](http://hbase.apache.org/images/hbase_logo_with_orca_large.png)\n",
    "\n",
    "Esta hoja muestra cómo acceder a bases de datos HBase y también a conectar la salida con Jupyter.\n",
    "\n",
    "Se puede utilizar el *shell* propio de HBase en la máquina virtual llamando al programa:\n",
    "\n",
    "    $ hbase/bin/hbase shell\n",
    "\n",
    "La diferencia es que ese programa espera código Ruby y aquí trabajaremos con Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nota sobre la caída de RegionServers\n",
    "\n",
    "En este entorno con poca memoria son frecuentes las caídas de RegionServers. Sería conveniente:\n",
    "\n",
    "- dar a la memoria virtual al menos 3GB de memoria,\n",
    "- aumentar el tamaño del HEAP de los procesos de HBase, y\n",
    "- aumentar el tiempo de _timeout_ de Zookeeper.\n",
    "\n",
    "En la [documentación de HBase](http://hbase.apache.org/book.html#trouble.rs) dan unas recomendaciones, sobre todo, para carga inicial, como he realizado estos días para cargar las bases de datos de ejemplo:\n",
    "\n",
    "> Make sure you give plenty of RAM (in hbase-env.sh), the default of 1GB won’t be able to sustain long running imports.\n",
    ">\n",
    "> [...]\n",
    ">\n",
    "> If this is happening during an upload which only happens once (like initially loading all your data into HBase), consider bulk loading.\n",
    "\n",
    "Aunque no usamos _bulk loading_ para mostrar cómo se añaden datos desde Python (el _bulk loading_ hay que hacerlo en Java).\n",
    "\n",
    "Las caídas en los RegionServers pueden producirse por varias cuestiones: falta de memoria, timeout por la ejecución del GC de Java, etc. Estas caídas son aceptadas como normales por el sistema HBase, que continuará funcionando con el resto de RegionServers y aceptará un RegionServer que terminó abruptamente una vez reiniciado.\n",
    "\n",
    "En nuestra VM sólo hay un RegionServer, y se puede iniciar si cayó con el comando:\n",
    "\n",
    "    ~/hbase/bin/start-daemon.sh start regionserver\n",
    "\n",
    "El siguiente _script_ comprueba cada 30 segundos la salida de depuración del Máster de HBase, y si ve que no hay RegionServers, llama al script de reinicio del único RegionServer. El cliente continuará sin problemas después de unos segundos de inicialización. Al cabo del tiempo, los RegionServer que no funcionan se eliminan por HBase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile restart-regionserver.sh\n",
    "#! /bin/sh\n",
    "while true ; do\n",
    "\tsleep 30 # Sleep before to give time HBase to start\n",
    "\tns=`curl -s http://localhost:60010/jmx | grep numRegionServers | tr -cd [0-9]`\n",
    "\ttest -z \"$ns\" || test $ns -gt 0 || ~/hbase/bin/hbase-daemon.sh start regionserver \n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memoria de intercambio\n",
    "\n",
    "El tamaño de la memoria que requiere puntualmente HBase hace que tengamos que crear un fichero de intercambio si no existe, y activarla. Se usarán 4GB para permitir el uso de memoria. Esto hará el sistema lento en caso de que tenga que hacer uso del intercambio, pero al menos no morirán por falta de memoria los distintos servidores de HBase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! sudo grep /swap /proc/swaps 2>&1 >/dev/null\n",
    "then\n",
    "    sudo fallocate -l 4GiB /swap\n",
    "    sudo chmod 0600 /swap\n",
    "    sudo mkswap /swap\n",
    "    sudo swapon /swap\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniciamos HBase. Esto lanza todos los demonios y el demonio de HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "~/start-hbase.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniciamos el script en segundo plano para que reinicie los regionservers que se caen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "sh restart-regionserver.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "%env DIR=/vagrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint as pp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos la librería `happybase` para python. La cargamos a continuación y hacemos la conexión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import happybase\n",
    "\n",
    "host = '127.0.0.1'\n",
    "connection = happybase.Connection(host)\n",
    "connection.tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "(test -e $DIR/Posts.csv && echo \"Ya descargado\") || (\\\n",
    "(wget http://neuromancer.inf.um.es:8080/es.stackoverflow/Posts.csv.gz -O - 2>/dev/null | gunzip > $DIR/Posts.csv) \\\n",
    "  && echo OK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "(test -e $DIR/Users.csv && echo \"Ya descargado\") || (\\\n",
    "(wget http://neuromancer.inf.um.es:8080/es.stackoverflow/Users.csv.gz -O - 2>/dev/null | gunzip > $DIR/Users.csv) \\\n",
    "  && echo OK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "(test -e $DIR/Tags.csv && echo \"Ya descargado\") || (\\\n",
    "(wget http://neuromancer.inf.um.es:8080/es.stackoverflow/Tags.csv.gz -O - 2>/dev/null | gunzip > $DIR/Tags.csv) \\\n",
    "  && echo OK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "(test -e $DIR/Comments.csv && echo \"Ya descargado\") || (\\\n",
    "(wget http://neuromancer.inf.um.es:8080/es.stackoverflow/Comments.csv.gz -O - 2>/dev/null | gunzip > $DIR/Comments.csv) \\\n",
    "  && echo OK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "(test -e $DIR/Votes.csv && echo \"Ya descargado\") || (\\\n",
    "(wget http://neuromancer.inf.um.es:8080/es.stackoverflow/Votes.csv.gz -O - 2>/dev/null | gunzip > $DIR/Votes.csv) \\\n",
    "  && echo OK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "(test -e $DIR/PostHistory.csv && echo \"Ya descargado\") || (\\\n",
    "(wget http://neuromancer.inf.um.es:8080/es.stackoverflow/PostHistory.csv.gz -O - 2>/dev/null | gunzip > $DIR/PostHistory.csv) \\\n",
    "  && echo OK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la carga inicial, vamos a crear todas las tablas con una única familia de columnas, `rawdata`, donde meteremos toda la información _raw_ comprimida. Después podremos hacer reorganizaciones de los datos para hacer el acceso más eficiente. Es una de las muchas ventajas de no tener un esquema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create tables\n",
    "tables = ['posts', 'votes', 'users', 'tags', 'comments']\n",
    "for t in tables:\n",
    "    try:\n",
    "        connection.create_table(\n",
    "            t,\n",
    "            {\n",
    "                'rawdata': dict(max_versions=1,compression='GZ')\n",
    "            })\n",
    "    except:\n",
    "        print \"Database already exists: {0}.\".format(t)\n",
    "        pass\n",
    "connection.tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código de importación es siempre el mismo, ya que se coge la primera fila del CSV que contiene el nombre de las columnas y se utiliza para generar nombres de columnas dentro de la familia de columnas dada como parámetro. La función `csv_to_hbase()` acepta un fichero CSV a abrir, un nombre de tabla y una familia de columnas donde agregar las columnas del fichero CSV. En nuestro caso siempre va a ser `rawdata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def csv_to_hbase(file, tablename, cf):\n",
    "    table = connection.table(tablename)\n",
    "    \n",
    "    with open(file) as f:\n",
    "        # La llamada csv.reader() crea un iterador sobre un fichero CSV\n",
    "        reader = csv.reader(f, dialect='excel')\n",
    "        \n",
    "        # Se leen las columnas. Sus nombres se usarán para crear las diferentes columnas en la familia\n",
    "        columns = reader.next()\n",
    "        columns = [cf + ':' + c for c in columns]\n",
    "        \n",
    "        with table.batch(batch_size=50) as b:\n",
    "            for row in reader:\n",
    "                # La primera columna se usará como Row Key\n",
    "                b.put(row[0], dict(zip(columns[1:], row[1:])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for t in tables:\n",
    "    print \"Importando tabla {0}...\".format(t)\n",
    "    %time csv_to_hbase(os.environ['DIR']+'/'+ t.capitalize() + '.csv', t, 'rawdata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcción de estructuras anidadas\n",
    "\n",
    "Al igual que pasaba con MongoDB, las bases de datos NoSQL como en este caso HBase permiten almacenar estructuras de datos complejas. En nuestro caso vamos a agregar los comentarios de cada pregunta o respuesta (post) en columnas del mismo. Para ello, creamos una nueva familia de columnas `comments`.\n",
    "\n",
    "HBase es bueno para añadir columnas sencillas, por ejemplo que contengan un valor. Sin embargo, si queremos añadir objetos complejos, tenemos que jugar con la codificación de la familia de columnas y columna.\n",
    "\n",
    "Usaremos el shell porque `happybase` no permite alterar tablas ya creadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<EOF | ~/hbase/bin/hbase shell\n",
    "\n",
    "disable 'posts'\n",
    "\n",
    "alter 'posts', {NAME => 'comments', VERSIONS => 1}\n",
    "\n",
    "enable 'posts'\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada comentario que añadimos contiene, al menos:\n",
    "\n",
    "- un id único\n",
    "- un texto\n",
    "- un autor\n",
    "- etc.\n",
    "\n",
    "¿Cómo se consigue meterlo en una única familia de columnas?\n",
    "\n",
    "Hay varias formas. La que usaremos aquí, añadiremos el **id** de cada comentario como parte del nombre de la columna. Por ejemplo, el comentario con Id 2000, generará las columnas:\n",
    "\n",
    "- `Id_2000` (valor 2000)\n",
    "- `UserId_2000`\n",
    "- `PostId_2000`\n",
    "- `Text_2000`\n",
    "\n",
    "con sus correspondientes valores. Así, todos los datos relativos al comentario con Id original 2000, estarán almacenados en todas las columnas que termienn en `_2000`. La base de datos permite implementar filtros que nos permiten buscar esto de forma muy sencilla. Los veremos después."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comments = connection.table('comments')\n",
    "posts = connection.table('posts')\n",
    "\n",
    "with posts.batch(batch_size=50) as b:\n",
    "    # Hacer un scan de la tabla\n",
    "    for key, data in comments.scan():\n",
    "        comment = {'comments:' + d.split(':')[1] + \"_\" + str(key): data[d] for d in data.keys()}\n",
    "        b.put(data['rawdata:PostId'], comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código permite mostrar de forma amigable las tablas extraídas de la base de datos en forma de diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/a/30525061/62365\n",
    "class DictTable(dict):\n",
    "    # Overridden dict class which takes a dict in the form {'a': 2, 'b': 3},\n",
    "    # and renders an HTML Table in IPython Notebook.\n",
    "    def _repr_html_(self):\n",
    "        html = [\"<table width=100%>\"]\n",
    "        for key, value in self.iteritems():\n",
    "            html.append(\"<tr>\")\n",
    "            html.append(\"<td>{0}</td>\".format(key))\n",
    "            html.append(\"<td>{0}</td>\".format(value))\n",
    "            html.append(\"</tr>\")\n",
    "        html.append(\"</table>\")\n",
    "        return ''.join(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Muestra cómo queda la fila del Id del Post 9997\n",
    "DictTable(posts.row('9997'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO: ¿Cómo sería el código para saber qué usuarios han comentado un post en particular?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia\n",
    "\n",
    "Como otro ejemplo de carga de datos y de organización en HBase, veremos de manera simplificada el ejemplo de la wikipedia visto en teoría.\n",
    "\n",
    "A continuación se descarga una pequeña parte del fichero de la wikipedia en XML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "FILE=eswiki.xml\n",
    "(test -e $DIR/$FILE && echo \"Ya descargado\") || (\\\n",
    "(wget http://neuromancer.inf.um.es:8080/wikipedia/$FILE.gz -O - 2>/dev/null | gunzip > $DIR/$FILE) \\\n",
    "  && echo OK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -200 $DIR/eswiki.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea la tabla para albergar la `wikipedia`. Igual que la vista en teoría, pero aquí se usa `wikipedia` en vez de `wiki` para que no colisionen la versión completa con la reducida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<EOF | ~/hbase/bin/hbase shell\n",
    "create 'wikipedia' , 'text', 'revision'\n",
    "\n",
    "disable 'wikipedia' # Para evitar su uso temporal\n",
    "\n",
    "alter 'wikipedia' , { NAME => 'text', VERSIONS => org.apache.hadoop.hbase.HConstants::ALL_VERSIONS }\n",
    "\n",
    "alter 'wikipedia' , { NAME => 'revision', VERSIONS => org.apache.hadoop.hbase.HConstants::ALL_VERSIONS }\n",
    "\n",
    "alter 'wikipedia' , { NAME => 'text', COMPRESSION => 'GZ', BLOOMFILTER => 'ROW'}\n",
    "\n",
    "enable 'wikipedia'\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código, visto en teoría, recorre el árbol XML construyendo documentos y llamando a la función `callback` con cada uno. Los documentos son diccionarios con las claves encontradas dentro de los tags `<page>...</page>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.sax\n",
    "import re\n",
    "\n",
    "class WikiHandler(xml.sax.handler.ContentHandler):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._charBuffer = ''\n",
    "        self.document = {}\n",
    "\n",
    "    def _getCharacterData(self):\n",
    "        data = self._charBuffer\n",
    "        self._charBuffer = ''\n",
    "        return data\n",
    "\n",
    "    def parse(self, f, callback):\n",
    "        self.callback = callback\n",
    "        xml.sax.parse(f, self)\n",
    "\n",
    "    def characters(self, data):\n",
    "        self._charBuffer = self._charBuffer + data\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        if name == 'page':\n",
    "        # print 'Start of page'\n",
    "            self.document = {}\n",
    "        if re.match(r'title|timestamp|username|comment|text', name):\n",
    "            self._charBuffer = ''\n",
    "\n",
    "    def endElement(self, name):\n",
    "        if re.match(r'title|timestamp|username|comment|text', name):\n",
    "            self.document[name] = self._getCharacterData()\n",
    "            # print(name, ': ', self.document[name][:20])\n",
    "        if 'revision' == name:\n",
    "            self.callback(self.document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El codigo a continuación, cada vez que el código anterior llama a la función `processdoc()` se añade un documento a la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class FillWikiTable():\n",
    "    \"\"\"Llena la tabla Wiki\"\"\"\n",
    "    def __init__(self):\n",
    "        # Conectar a la base de datos a través de Thrift\n",
    "        self.table = connection.table('wikipedia')\n",
    "\n",
    "    def run(_s):\n",
    "        def processdoc(d):\n",
    "            print \"Callback called with\", d['title']\n",
    "            tuple_time = time.strptime(d['timestamp'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            timestamp = int(time.mktime(tuple_time))\n",
    "            _s.table.put(d['title'],\n",
    "                         {'text:': d.get('text',''),\n",
    "                          'revision:author': d.get('username',''),\n",
    "                          'revision:comment': d.get('comment','')},\n",
    "                         timestamp=timestamp)\n",
    "\n",
    "        with open(os.environ['DIR']+'/'+'eswiki.xml','rb') as f:\n",
    "            start = time.time()\n",
    "            WikiHandler().parse(f, processdoc)\n",
    "            end = time.time()\n",
    "            print (\"End adding documents. Time: %.5f\" % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FillWikiTable().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código a continuación permite ver las diferentes versiones de una revisión. Como la versión reducida es muy pequeña no da lugar a que haya ninguna revisión, pero con este código se vería. Hace uso del _shell_ de HBase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<EOF | ~/hbase/bin/hbase shell\n",
    "\n",
    "get 'wikipedia', 'Commodore Amiga', {COLUMN => 'revision',VERSIONS=>10}\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enlazado de documentos en la wikipedia\n",
    "\n",
    "Los artículos de la wikipedia llevan enlaces entre sí, incluyendo referencias del tipo `[[artículo referenciado]]`. Se pueden extraer estos enlaces y se puede construir un grafo de conexiones. Para cada artículo, se anotarán qué enlaces hay que salen de él y hacia qué otros artículos enlazan y también qué enlaces llegan a él. Esto se hará con dos familias de columnas, `from` y `to`. \n",
    "\n",
    "En cada momento, se añadirá una columna `from:artículo` cuando un artículo nos apunte, y otras columnas `to:articulo` con los artículos que nosotros enlazamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "class BuildLinks():\n",
    "    \"\"\"Llena la tabla de Links\"\"\"\n",
    "    def __init__(self):\n",
    "        # Create table\n",
    "        try:\n",
    "            connection.create_table(\n",
    "                \"wikilinks\",\n",
    "                {\n",
    "                    'from': dict(bloom_filter_type='ROW',max_versions=1),\n",
    "                    'to' : dict(bloom_filter_type='ROW',max_versions=1)\n",
    "                })\n",
    "        except:\n",
    "            print (\"Database wikilinks already exists.\")\n",
    "            pass\n",
    "\n",
    "        self.table = connection.table('wikilinks')\n",
    "        self.wikitable = connection.table('wikipedia')\n",
    "\n",
    "    def run(self):\n",
    "        print \"run\";\n",
    "        linkpattern = r'\\[\\[([^\\[\\]\\|\\:\\#][^\\[\\]\\|:]*)(?:\\|([^\\[\\]\\|]+))?\\]\\]'\n",
    "        # target, label\n",
    "\n",
    "        with self.table.batch(batch_size=50) as b:\n",
    "            for key, data in self.wikitable.scan():\n",
    "                to_dict = {}\n",
    "                doc = key.strip()\n",
    "                print \"\\n\", doc, \":\"\n",
    "                for mo in re.finditer(linkpattern, data['text:']):\n",
    "                    (target, label) = mo.groups()\n",
    "\n",
    "                    target = target.strip()\n",
    "\n",
    "                    if target == '':\n",
    "                        continue\n",
    "\n",
    "                    label = '' if not label else label\n",
    "                    label = label.strip()\n",
    "\n",
    "                    to_dict['to:' + target] = label\n",
    "\n",
    "                    sys.stdout.write(\".\")\n",
    "                    #print \"%s -> %s (%s)\" % (doc, target, label)\n",
    "                    \n",
    "                    b.put(target, {'from:' + doc : label})\n",
    "\n",
    "                if bool(to_dict):\n",
    "                    b.put(doc, to_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BuildLinks().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente sesión veremos técnicas más sofisticadas de filtrado, pero por ahora se puede jugar con estas construcciones. Se puede seleccionar qué columnas se quiere mostrar e incluso filtros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<EOF | ~/hbase/bin/hbase shell\n",
    "\n",
    "scan 'wikilinks', {COLUMNS=>'to', FILTER => \"ColumnPrefixFilter('A')\", LIMIT => 300}\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proceso de `scan` recorre toda la tabla mostrando sólo las filas seleccionadas. HBase ofrece ciertas optimizaciones para que el escaneo sea eficiente, que veremos en la siguiente sesión.\n",
    "\n",
    "Una introducción a los filtros y parámetros disponibles se puede ver [aquí](http://www.hadooptpoint.com/filters-in-hbase-shell/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<EOF | ~/hbase/bin/hbase shell\n",
    "\n",
    "scan 'wikipedia', {COLUMNS=>['revision'] , STARTROW => 'A', ENDROW=>'B'}\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO: Encontrar páginas que estén enlazadas y que ambas estén en la tabla `wikipedia`\n",
    "\n",
    "(Ojo, no estarán todas porque es una versión reducida de la wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO: Probar diversas búsquedas sobre las tablas `wikipedia` y `wikilinks`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO: Modificar la tabla `posts` para añadir una familia de columnas que guarde el histórico de ediciones guardado en `PostHistory.csv`. Usar como ejemplo la función csv_to_hbase"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
